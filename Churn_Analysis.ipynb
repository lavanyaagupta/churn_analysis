{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959e24cb-25ef-4288-ab6f-001ec25d94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Churn Analysis - Data Science Project\n",
    "# Author: Lavanyaa Gupta\n",
    "# Description: A comprehensive analysis of customer churn using machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5a08f-3eca-4345-9240-39b12324a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c23460-b1d6-47b7-a703-6589b08ed859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classes\n",
    "class CustomerChurnAnalyzer:\n",
    "    \"\"\"\n",
    "    A comprehensive customer churn analysis tool that demonstrates\n",
    "    key data science skills including EDA, feature engineering, and ML modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def generate_sample_data(self, n_samples=1000):\n",
    "        \"\"\"Generate realistic customer data for demonstration purposes.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate customer features\n",
    "        data = {\n",
    "            'customer_id': range(1, n_samples + 1),\n",
    "            'age': np.random.normal(45, 15, n_samples).astype(int),\n",
    "            'tenure_months': np.random.exponential(24, n_samples).astype(int),\n",
    "            'monthly_charges': np.random.normal(65, 20, n_samples),\n",
    "            'total_charges': np.random.normal(1500, 800, n_samples),\n",
    "            'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], \n",
    "                                            n_samples, p=[0.5, 0.3, 0.2]),\n",
    "            'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], \n",
    "                                               n_samples, p=[0.4, 0.4, 0.2]),\n",
    "            'tech_support': np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7]),\n",
    "            'payment_method': np.random.choice(['Electronic check', 'Mailed check', \n",
    "                                              'Bank transfer', 'Credit card'], \n",
    "                                            n_samples, p=[0.4, 0.2, 0.2, 0.2])\n",
    "        }\n",
    "        \n",
    "        # Create realistic churn based on features\n",
    "        churn_prob = (\n",
    "            0.1 +  # Base churn rate\n",
    "            0.3 * (data['contract_type'] == 'Month-to-month') +\n",
    "            0.2 * (data['payment_method'] == 'Electronic check') +\n",
    "            0.15 * (data['tech_support'] == 'No') +\n",
    "            0.1 * (np.array(data['monthly_charges']) > 80) +\n",
    "            -0.2 * (np.array(data['tenure_months']) > 24)\n",
    "        )\n",
    "        \n",
    "        data['churn'] = np.random.binomial(1, np.clip(churn_prob, 0, 1), n_samples)\n",
    "        \n",
    "        self.df = pd.DataFrame(data)\n",
    "        \n",
    "        # Add some missing values for realistic data cleaning demo\n",
    "        missing_indices = np.random.choice(self.df.index, size=int(0.05 * n_samples), replace=False)\n",
    "        self.df.loc[missing_indices, 'total_charges'] = np.nan\n",
    "        \n",
    "        print(f\"Generated dataset with {n_samples} customers\")\n",
    "        return self.df\n",
    "    \n",
    "    def load_data(self, filepath=None):\n",
    "        \"\"\"Load data from file or generate sample data.\"\"\"\n",
    "        if filepath:\n",
    "            try:\n",
    "                self.df = pd.read_csv(filepath)\n",
    "                print(f\"Loaded data from {filepath}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File {filepath} not found. Generating sample data instead.\")\n",
    "                self.generate_sample_data()\n",
    "        else:\n",
    "            self.generate_sample_data()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def exploratory_data_analysis(self):\n",
    "        \"\"\"Perform comprehensive exploratory data analysis.\"\"\"\n",
    "        print(\"=== EXPLORATORY DATA ANALYSIS ===\\n\")\n",
    "        \n",
    "        # Basic info\n",
    "        print(\"Dataset Shape:\", self.df.shape)\n",
    "        print(\"\\nData Types:\")\n",
    "        print(self.df.dtypes)\n",
    "        \n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(self.df.isnull().sum())\n",
    "        \n",
    "        print(\"\\nChurn Distribution:\")\n",
    "        churn_counts = self.df['churn'].value_counts()\n",
    "        print(churn_counts)\n",
    "        print(f\"Churn Rate: {churn_counts[1] / len(self.df) * 100:.2f}%\")\n",
    "        \n",
    "        # Statistical summary\n",
    "        print(\"\\nNumerical Features Summary:\")\n",
    "        print(self.df.describe())\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Churn distribution\n",
    "        self.df['churn'].value_counts().plot(kind='bar', ax=axes[0,0], color=['skyblue', 'salmon'])\n",
    "        axes[0,0].set_title('Churn Distribution')\n",
    "        axes[0,0].set_xlabel('Churn (0=No, 1=Yes)')\n",
    "        \n",
    "        # Age distribution by churn\n",
    "        self.df.boxplot(column='age', by='churn', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Age Distribution by Churn')\n",
    "        \n",
    "        # Monthly charges by churn\n",
    "        self.df.boxplot(column='monthly_charges', by='churn', ax=axes[0,2])\n",
    "        axes[0,2].set_title('Monthly Charges by Churn')\n",
    "        \n",
    "        # Tenure distribution\n",
    "        self.df['tenure_months'].hist(bins=30, ax=axes[1,0], alpha=0.7, color='green')\n",
    "        axes[1,0].set_title('Tenure Distribution')\n",
    "        axes[1,0].set_xlabel('Tenure (months)')\n",
    "        \n",
    "        # Contract type vs churn\n",
    "        pd.crosstab(self.df['contract_type'], self.df['churn']).plot(kind='bar', \n",
    "                                                                    ax=axes[1,1], \n",
    "                                                                    color=['skyblue', 'salmon'])\n",
    "        axes[1,1].set_title('Contract Type vs Churn')\n",
    "        axes[1,1].legend(['No Churn', 'Churn'])\n",
    "        \n",
    "        # Correlation heatmap\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        corr_matrix = self.df[numeric_cols].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,2])\n",
    "        axes[1,2].set_title('Feature Correlation Matrix')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Clean and preprocess the data for machine learning.\"\"\"\n",
    "        print(\"\\n=== DATA PREPROCESSING ===\\n\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        print(\"Handling missing values...\")\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        self.df['total_charges'] = imputer.fit_transform(self.df[['total_charges']])\n",
    "        \n",
    "        # Create feature engineering\n",
    "        print(\"Engineering new features...\")\n",
    "        self.df['avg_monthly_charges'] = self.df['total_charges'] / (self.df['tenure_months'] + 1)\n",
    "        self.df['high_value_customer'] = (self.df['monthly_charges'] > self.df['monthly_charges'].quantile(0.75)).astype(int)\n",
    "        self.df['long_tenure'] = (self.df['tenure_months'] > 24).astype(int)\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        print(\"Encoding categorical variables...\")\n",
    "        le = LabelEncoder()\n",
    "        categorical_cols = ['contract_type', 'internet_service', 'tech_support', 'payment_method']\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            self.df[col + '_encoded'] = le.fit_transform(self.df[col])\n",
    "        \n",
    "        # Prepare features and target\n",
    "        feature_cols = ['age', 'tenure_months', 'monthly_charges', 'total_charges', \n",
    "                       'avg_monthly_charges', 'high_value_customer', 'long_tenure'] + \\\n",
    "                       [col + '_encoded' for col in categorical_cols]\n",
    "        \n",
    "        X = self.df[feature_cols]\n",
    "        y = self.df['churn']\n",
    "        \n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        print(f\"Training set size: {self.X_train.shape}\")\n",
    "        print(f\"Test set size: {self.X_test.shape}\")\n",
    "        \n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def train_models(self):\n",
    "        \"\"\"Train multiple machine learning models.\"\"\"\n",
    "        print(\"\\n=== MODEL TRAINING ===\\n\")\n",
    "        \n",
    "        # Define models\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42),\n",
    "            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        for name, model in models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            \n",
    "            # Use scaled data for Logistic Regression, original for Random Forest\n",
    "            if name == 'Logistic Regression':\n",
    "                X_train_use = self.X_train_scaled\n",
    "                X_test_use = self.X_test_scaled\n",
    "            else:\n",
    "                X_train_use = self.X_train\n",
    "                X_test_use = self.X_test\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_use, self.y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test_use)\n",
    "            y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            cv_scores = cross_val_score(model, X_train_use, self.y_train, cv=5, scoring='roc_auc')\n",
    "            roc_auc = roc_auc_score(self.y_test, y_pred_proba)\n",
    "            \n",
    "            # Store results\n",
    "            self.models[name] = model\n",
    "            self.results[name] = {\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_pred_proba,\n",
    "                'cv_scores': cv_scores,\n",
    "                'roc_auc': roc_auc,\n",
    "                'X_test': X_test_use\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - Cross-validation AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "            print(f\"{name} - Test AUC: {roc_auc:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluate and compare model performance.\"\"\"\n",
    "        print(\"\\n=== MODEL EVALUATION ===\\n\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # ROC Curves\n",
    "        for name, results in self.results.items():\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, results['probabilities'])\n",
    "            axes[0,0].plot(fpr, tpr, label=f\"{name} (AUC = {results['roc_auc']:.3f})\")\n",
    "        \n",
    "        axes[0,0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        axes[0,0].set_xlabel('False Positive Rate')\n",
    "        axes[0,0].set_ylabel('True Positive Rate')\n",
    "        axes[0,0].set_title('ROC Curves')\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Feature Importance (Random Forest)\n",
    "        if 'Random Forest' in self.models:\n",
    "            rf_model = self.models['Random Forest']\n",
    "            feature_names = self.X_train.columns\n",
    "            importances = rf_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]  # Top 10 features\n",
    "            \n",
    "            axes[0,1].bar(range(len(indices)), importances[indices])\n",
    "            axes[0,1].set_xlabel('Features')\n",
    "            axes[0,1].set_ylabel('Importance')\n",
    "            axes[0,1].set_title('Top 10 Feature Importances (Random Forest)')\n",
    "            axes[0,1].set_xticks(range(len(indices)))\n",
    "            axes[0,1].set_xticklabels([feature_names[i] for i in indices], rotation=45)\n",
    "        \n",
    "        # Confusion Matrix for best model\n",
    "        best_model_name = max(self.results.keys(), key=lambda x: self.results[x]['roc_auc'])\n",
    "        cm = confusion_matrix(self.y_test, self.results[best_model_name]['predictions'])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,0])\n",
    "        axes[1,0].set_xlabel('Predicted')\n",
    "        axes[1,0].set_ylabel('Actual')\n",
    "        axes[1,0].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "        \n",
    "        # Model Comparison\n",
    "        model_names = list(self.results.keys())\n",
    "        auc_scores = [self.results[name]['roc_auc'] for name in model_names]\n",
    "        cv_means = [self.results[name]['cv_scores'].mean() for name in model_names]\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[1,1].bar(x - width/2, auc_scores, width, label='Test AUC', color='skyblue')\n",
    "        axes[1,1].bar(x + width/2, cv_means, width, label='CV AUC', color='salmon')\n",
    "        axes[1,1].set_xlabel('Models')\n",
    "        axes[1,1].set_ylabel('AUC Score')\n",
    "        axes[1,1].set_title('Model Performance Comparison')\n",
    "        axes[1,1].set_xticks(x)\n",
    "        axes[1,1].set_xticklabels(model_names)\n",
    "        axes[1,1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(\"Detailed Classification Reports:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for name, results in self.results.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(classification_report(self.y_test, results['predictions']))\n",
    "    \n",
    "    def generate_insights(self):\n",
    "        \"\"\"Generate business insights from the analysis.\"\"\"\n",
    "        print(\"\\n=== BUSINESS INSIGHTS ===\\n\")\n",
    "        \n",
    "        insights = []\n",
    "        \n",
    "        # Churn rate analysis\n",
    "        churn_rate = self.df['churn'].mean() * 100\n",
    "        insights.append(f\"Overall churn rate is {churn_rate:.1f}%\")\n",
    "        \n",
    "        # Contract type impact\n",
    "        contract_churn = self.df.groupby('contract_type')['churn'].mean()\n",
    "        worst_contract = contract_churn.idxmax()\n",
    "        insights.append(f\"'{worst_contract}' contracts have the highest churn rate at {contract_churn[worst_contract]*100:.1f}%\")\n",
    "        \n",
    "        # Payment method impact\n",
    "        payment_churn = self.df.groupby('payment_method')['churn'].mean()\n",
    "        worst_payment = payment_churn.idxmax()\n",
    "        insights.append(f\"Customers using '{worst_payment}' have the highest churn rate at {payment_churn[worst_payment]*100:.1f}%\")\n",
    "        \n",
    "        # High-value customer analysis\n",
    "        if 'high_value_customer' in self.df.columns:\n",
    "            high_value_churn = self.df[self.df['high_value_customer'] == 1]['churn'].mean()\n",
    "            low_value_churn = self.df[self.df['high_value_customer'] == 0]['churn'].mean()\n",
    "            insights.append(f\"High-value customers churn at {high_value_churn*100:.1f}% vs {low_value_churn*100:.1f}% for others\")\n",
    "        \n",
    "        # Model performance insight\n",
    "        best_model_name = max(self.results.keys(), key=lambda x: self.results[x]['roc_auc'])\n",
    "        best_auc = self.results[best_model_name]['roc_auc']\n",
    "        insights.append(f\"Best performing model is {best_model_name} with AUC of {best_auc:.3f}\")\n",
    "        \n",
    "        print(\"Key Insights:\")\n",
    "        for i, insight in enumerate(insights, 1):\n",
    "            print(f\"{i}. {insight}\")\n",
    "        \n",
    "        print(\"\\nRecommendations:\")\n",
    "        print(\"1. Focus retention efforts on month-to-month contract customers\")\n",
    "        print(\"2. Investigate payment method preferences and offer incentives for stable methods\")\n",
    "        print(\"3. Implement early warning systems using the trained model\")\n",
    "        print(\"4. Develop targeted retention campaigns for high-risk customer segments\")\n",
    "    \n",
    "    def run_full_analysis(self, filepath=None):\n",
    "        \"\"\"Run the complete analysis pipeline.\"\"\"\n",
    "        print(\"Starting Customer Churn Analysis...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data(filepath)\n",
    "        \n",
    "        # EDA\n",
    "        self.exploratory_data_analysis()\n",
    "        \n",
    "        # Preprocessing\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # Model training\n",
    "        self.train_models()\n",
    "        \n",
    "        # Evaluation\n",
    "        self.evaluate_models()\n",
    "        \n",
    "        # Insights\n",
    "        self.generate_insights()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Analysis Complete!\")\n",
    "        \n",
    "        return self.df, self.models, self.results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis.\"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = CustomerChurnAnalyzer()\n",
    "    \n",
    "    # Run full analysis\n",
    "    df, models, results = analyzer.run_full_analysis()\n",
    "    \n",
    "    # Optional: Save results\n",
    "    # df.to_csv('processed_customer_data.csv', index=False)\n",
    "    # print(\"\\nResults saved to 'processed_customer_data.csv'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4c1b5-aabe-4ef4-8f21-ffe83bf91acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Examples:\n",
    "\n",
    "# 1. Basic usage:\n",
    "#    analyzer = CustomerChurnAnalyzer()\n",
    "#    analyzer.run_full_analysis()\n",
    "\n",
    "# 2. With your own data:\n",
    "#    analyzer = CustomerChurnAnalyzer()\n",
    "#    analyzer.run_full_analysis('your_data.csv')\n",
    "\n",
    "# 3. Step by step:\n",
    "#    analyzer = CustomerChurnAnalyzer()\n",
    "#    analyzer.load_data()\n",
    "#    analyzer.exploratory_data_analysis()\n",
    "#    analyzer.preprocess_data()\n",
    "#    analyzer.train_models()\n",
    "#    analyzer.evaluate_models()\n",
    "#    analyzer.generate_insights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
